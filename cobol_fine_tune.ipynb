{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets accelerate - -quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load your original JSON file\n",
    "with open('/Users/kjevaji/Code/jupyter/output/output_2.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Open a new JSONL file for writing\n",
    "with open('/Users/kjevaji/Code/jupyter/output/cobol_finetune_data_2.jsonl', 'w') as f_out:\n",
    "    for entry in data:\n",
    "        # Structure each line in JSONL format\n",
    "        json_line = {\n",
    "            \"prompt\": entry[\"code\"],\n",
    "            \"completion\": entry[\"comments\"]\n",
    "        }\n",
    "        f_out.write(json.dumps(json_line) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from JSONL\n",
    "dataset = load_dataset('json', data_files={\n",
    "                       'train': './output/cobol_finetune_data.jsonl'})\n",
    "print(dataset['train'][0])  # Print first training example to confirm structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    # Or use tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize function with padding\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples['prompt'], padding=\"max_length\", truncation=True, max_length=256)\n",
    "    outputs = tokenizer(\n",
    "        examples['completion'], padding=\"max_length\", truncation=True, max_length=256)\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"labels\": outputs[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Padding token:\", tokenizer.pad_token)\n",
    "print(\"Special tokens:\", tokenizer.special_tokens_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",  # Evaluate based on steps\n",
    "    save_steps=500,               # Save a checkpoint every 500 steps\n",
    "    eval_steps=500,               # Evaluate every 500 steps\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    no_cuda=True  # Force training on CPU\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and validation\n",
    "split_dataset = tokenized_datasets[\"train\"].train_test_split(\n",
    "    test_size=0.1)  # 10% for validation\n",
    "\n",
    "# Assign the train and validation sets\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "validation_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(train_dataset[0])\n",
    "print(validation_dataset[0])\n",
    "\n",
    "train_dataset = train_dataset.select(\n",
    "    range(len(train_dataset) // 100))  # Use only half the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "# Shuffle the training dataset to ensure randomness\n",
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "\n",
    "# Calculate the number of samples to use\n",
    "sample_size = len(train_dataset) // 100\n",
    "\n",
    "# Select a random sample from the dataset\n",
    "small_train_dataset = train_dataset.select(range(sample_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Update Trainer with validation dataset\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=small_train_dataset,\n",
    "#     eval_dataset=validation_dataset\n",
    "# )\n",
    "\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"./fine_tuned_llama_cobol\")\n",
    "# tokenizer.save_pretrained(\"./fine_tuned_llama_cobol\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# # Load the fine-tuned model and tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_llama_cobol\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_llama_cobol\")\n",
    "\n",
    "# # Define a test prompt (COBOL code snippet)\n",
    "# test_prompt = \"\"\"\n",
    "# COBOL Code:\n",
    "# IDENTIFICATION DIVISION.\n",
    "# PROGRAM-ID. HELLO-WORLD.\n",
    "# PROCEDURE DIVISION.\n",
    "#     DISPLAY 'Hello, world!'.\n",
    "#     STOP RUN.\n",
    "\n",
    "# Please generate detailed documentation for the above COBOL code:\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# # Encode the input and generate output\n",
    "# inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "# outputs = model.generate(**inputs, max_length=150)\n",
    "\n",
    "# # Decode and print the generated documentation\n",
    "# generated_doc = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# print(\"Generated Documentation:\")\n",
    "# print(generated_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# # Load the tokenizer and base model from Hugging Face\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# # Define a test prompt (COBOL code snippet)\n",
    "# test_prompt = \"\"\"\n",
    "# Example COBOL Code 1:\n",
    "# IDENTIFICATION DIVISION.\n",
    "# PROGRAM-ID. SAMPLE1.\n",
    "# PROCEDURE DIVISION.\n",
    "#     DISPLAY 'Sample 1 Program'.\n",
    "#     STOP RUN.\n",
    "\n",
    "# Documentation:\n",
    "# This COBOL program prints \"Sample 1 Program\" to the console. The IDENTIFICATION DIVISION identifies the program, and the PROCEDURE DIVISION contains the executable code.\n",
    "\n",
    "# Example COBOL Code 2:\n",
    "# IDENTIFICATION DIVISION.\n",
    "# PROGRAM-ID. SAMPLE2.\n",
    "# PROCEDURE DIVISION.\n",
    "#     DISPLAY 'Sample 2 Program'.\n",
    "#     STOP RUN.\n",
    "\n",
    "# Documentation:\n",
    "# This COBOL program prints \"Sample 2 Program\" to the console. The IDENTIFICATION DIVISION specifies the program name as SAMPLE2. The PROCEDURE DIVISION executes the DISPLAY command and then terminates the program.\n",
    "\n",
    "# Now generate documentation for the following COBOL code:\n",
    "# IDENTIFICATION DIVISION.\n",
    "# PROGRAM-ID. HELLO-WORLD.\n",
    "# PROCEDURE DIVISION.\n",
    "#     DISPLAY 'Hello, world!'.\n",
    "#     STOP RUN.\n",
    "# \"\"\"\n",
    "\n",
    "# # Tokenize and generate output\n",
    "# inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "# outputs = model.generate(**inputs, max_length=350)\n",
    "\n",
    "# # Decode and print the output\n",
    "# generated_doc = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# print(\"Generated Documentation:\")\n",
    "# print(generated_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/kjevaji/.cache/huggingface/datasets/json/default-1f8128608f5db8d4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1db1a3b63a4ba3b80b9e8f2b36077c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset keys: ['code', 'comments', 'description', 'type']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9eb1882fe141259f3c0a4a55bd0502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e225db30b147e59b9cced8c1e31cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1173.9183, 'train_samples_per_second': 0.046, 'train_steps_per_second': 0.046, 'train_loss': 0.8099677474410446, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_llama_cobol/tokenizer_config.json',\n",
       " './fine_tuned_llama_cobol/special_tokens_map.json',\n",
       " './fine_tuned_llama_cobol/tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "# pip install transformers datasets accelerate\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Initialize Accelerator to use CPU\n",
    "accelerator = Accelerator(cpu=True)\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add or define the padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    # Use eos_token as pad_token, or alternatively:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Prepare model with accelerator (do not need to prepare datasets)\n",
    "model = accelerator.prepare(model)\n",
    "\n",
    "# Load your dataset from JSONL\n",
    "dataset = load_dataset('json', data_files={\n",
    "                       'train': '/Users/kjevaji/Code/jupyter/output/output_3_small.json'})\n",
    "\n",
    "# Debug: Print dataset keys to understand the structure\n",
    "print(\"Dataset keys:\", dataset[\"train\"].column_names)\n",
    "\n",
    "# Define a function to tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    # Combine type, description, code, and comments for the conversation context\n",
    "    inputs_texts = [\n",
    "        f\"**Type:** {type}\\n**Description:** {description}\\n**Code:** {code}\\n**Comments:** {comments}\"\n",
    "        for type, description, code, comments in zip(examples['type'], examples['description'], examples['code'], examples['comments'])\n",
    "    ]\n",
    "    \n",
    "    # Tokenize input texts\n",
    "    inputs = tokenizer(\n",
    "        inputs_texts, padding=\"max_length\", truncation=True, max_length=512\n",
    "    )\n",
    "    \n",
    "    # Create labels that are the same as inputs for auto-regressive training\n",
    "    labels = inputs[\"input_ids\"]\n",
    "\n",
    "    # Mask the input tokens corresponding to padding\n",
    "    labels = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in label]\n",
    "        for label in labels\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# Apply the tokenization function to the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Split the dataset into training and validation sets (90% train, 10% validation)\n",
    "split_dataset = tokenized_datasets[\"train\"].train_test_split(test_size=0.1)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "validation_dataset = split_dataset[\"test\"]\n",
    "\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "# Shuffle the training dataset to ensure randomness\n",
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "\n",
    "# Calculate the number of samples to use\n",
    "sample_size = len(train_dataset)\n",
    "\n",
    "# Select a random sample from the dataset\n",
    "small_train_dataset = train_dataset.select(range(sample_size))\n",
    "\n",
    "# Set up training arguments without mixed precision\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",  # Evaluate based on steps\n",
    "    save_steps=500,               # Save a checkpoint every 500 steps\n",
    "    eval_steps=500,               # Evaluate every 500 steps\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,  # Keep batch size small to avoid memory issues\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    use_cpu=True  # Force training on CPU\n",
    ")\n",
    "\n",
    "# Initialize the Trainer with model, arguments, and datasets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=validation_dataset\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"./fine_tuned_llama_cobol\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_llama_cobol\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Documentation:\n",
      "\n",
      "       IDENTIFICATION DIVISION.\n",
      "       PROGRAM-ID. HELLO-WORLD.\n",
      "       PROCEDURE DIVISION.\n",
      "           DISPLAY 'Hello, world!'.\n",
      "           STOP RUN.\n",
      "       \n",
      "       DATA DIVISION.\n",
      "           H-WORD-PROGRAM-ID.: HELLO-WORLD.\n",
      "           H-PROGRAM-DATA DIVISION.\n",
      "               PROGRAM-NAME: HELLO-WORLD.\n",
      "               PROGRAM-DESCRIPTION: This COBOL program is designed to display the\n",
      "               message \"Hello, world!\" on the screen.\n",
      "               H-OPER-DRG-CODE: 0000-0000.\n",
      "               H-OPER-DSH-CODE: 0000-0000.\n",
      "               H-OPER-DSH-DAYS-CODE: 0000-\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"\"\"\n",
    "COBOL Code:\n",
    "IDENTIFICATION DIVISION.\n",
    "PROGRAM-ID. HELLO-WORLD.\n",
    "PROCEDURE DIVISION.\n",
    "    DISPLAY 'Hello, world!'.\n",
    "    STOP RUN.\n",
    "\n",
    "Please generate detailed documentation for the above COBOL code:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_llama_cobol\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_llama_cobol\")\n",
    "\n",
    "# Set up a generation pipeline\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example COBOL code snippet to generate documentation for\n",
    "code_snippet = \"\"\"\n",
    "       IDENTIFICATION DIVISION.\n",
    "       PROGRAM-ID. HELLO-WORLD.\n",
    "       PROCEDURE DIVISION.\n",
    "           DISPLAY 'Hello, world!'.\n",
    "           STOP RUN.\n",
    "\"\"\"\n",
    "\n",
    "# Generate documentation\n",
    "generated_text = generator(code_snippet, max_length=150, num_return_sequences=1)\n",
    "print(\"Generated Documentation:\")\n",
    "print(generated_text[0]['generated_text'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
