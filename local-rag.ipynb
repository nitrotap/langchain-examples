{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "237cb5fd-3c84-4a3a-b6e0-c28825409122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain_ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b1624de-e4a5-418b-9d01-68918fbb73ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kjevaji/Code/jupyter/jupyter_env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'...Neil Armstrong. He stepped onto the lunar surface on July 20, 1969 as part of the Apollo 11 mission.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "llm.invoke(\"The first man on the moon was ...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dc0f2fd-bcdf-48b1-8ace-756a4cbcb87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...|Neil| Armstrong|.| He| stepped| onto| the| lunar| surface| on| July| |20|,| |196|9| as| part| of| the| Apollo| |11| mission|.||"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(\"The first man on the moon was ...\"):\n",
    "    print(chunk, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4ad6675-f7f8-4bac-9d05-37e91b3057e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The first man to walk on the Moon was Neil Armstrong. He stepped out of the lunar module Eagle and onto the Moon\\'s surface on July 20, 1969, during the Apollo 11 mission. His famous words upon setting foot on the Moon were: \"That\\'s one small step for man, one giant leap for mankind.\"', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2024-11-10T06:06:40.100411Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 945167667, 'load_duration': 27752000, 'prompt_eval_count': 34, 'prompt_eval_duration': 162000000, 'eval_count': 69, 'eval_duration': 754000000}, id='run-173be5d0-52f0-4a01-b47f-a54f5ca5448c-0', usage_metadata={'input_tokens': 34, 'output_tokens': 69, 'total_tokens': 103})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "chat_model = ChatOllama(model=\"llama3.2\")\n",
    "\n",
    "chat_model.invoke(\"Who was the first man on the moon?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfa48974-e70c-4ecb-9b48-10d591dc3f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Document loading, retrieval methods and text splitting\n",
    "%pip install -qU langchain langchain_community\n",
    "\n",
    "# Local vector store via Chroma\n",
    "%pip install -qU langchain_chroma\n",
    "\n",
    "# Local inference and embeddings via Ollama\n",
    "%pip install -qU langchain_ollama\n",
    "\n",
    "# Web Loader\n",
    "%pip install -qU beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b18a0b6-c1cd-4841-9f75-8dd5ac1cff94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "987c0ab8-e43b-4426-b34f-4f5f048540ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "local_embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=local_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a711736a-c7a5-43db-90be-581bfbff3b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6131a697-d68d-4248-a415-d794902c9745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b260cf2a-061a-492f-8b48-4d20da090898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1164d46-d20c-49e0-9118-4b85db48e2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(The scene is set in a dark, crowded nightclub. The audience is on the edge of their seats as two figures emerge from the shadows. On one side, we have Stephen Colbert, dressed in his signature suit and tie, with a confident smirk on his face. Across from him, John Oliver stands tall, his glasses perched atop his nose, and a hint of mischief in his eyes.)\n",
      "\n",
      "**Round 1: Stephen Colbert**\n",
      "\n",
      "Yo, I'm the king of late-night fame,\n",
      "Colbert Report's my claim to fame.\n",
      "I brought satire to the mainstream game,\n",
      "While you were still trying to get your name.\n",
      "\n",
      "My wit is sharp, my humor's tight,\n",
      "I made fun of politicians with all my might.\n",
      "From Bush to Trump, I took aim,\n",
      "Exposing the truth, no matter the claim.\n",
      "\n",
      "**Round 1: John Oliver**\n",
      "\n",
      "Hold up, hold up, let me interrupt,\n",
      "You think you're funny, but your jokes are incorrect.\n",
      "My Last Week Tonight's where the real talk is done,\n",
      "I tackle topics that make America numb.\n",
      "\n",
      "From voter ID to coal mining fees,\n",
      "I dive deep into issues that affect thee.\n",
      "No fluff, no filler – just raw, unadulterated truth,\n",
      "Leaving you in the dust, with your shallow youth.\n",
      "\n",
      "**Round 2: Stephen Colbert**\n",
      "\n",
      "Respectfully disagree, my friend so fine,\n",
      "My show's for all Americans, not just the elite's shrine.\n",
      "I cover topics from silly to sublime,\n",
      "Keeping it real, keeping it on my mind.\n",
      "\n",
      "You may think you're edgy, but I'm the one with flair,\n",
      "My show's a party, your show's more like a snare.\n",
      "Don't get me wrong, you've got skills galore,\n",
      "But when it comes to laughs and fun, I'm the one who scores.\n",
      "\n",
      "**Round 2: John Oliver**\n",
      "\n",
      "You may think you're funny, but humor's not just about flair,\n",
      "It's about substance, heart, and a willingness to care.\n",
      "My show's a movement, a call to action too,\n",
      "We tackle the tough stuff, so you don't have to.\n",
      "\n",
      "Your jokes are clever, but mine are bold,\n",
      "I expose the hypocrisy, leaving you cold.\n",
      "You may be the king of comedy fame,\n",
      "But when it comes to substance, I'm the one with the game.\n",
      "\n",
      "**Round 3: Stephen Colbert**\n",
      "\n",
      "Alright, alright, let's wrap this up tight,\n",
      "We both bring the heat, day and night.\n",
      "But if I had to choose a winner, I'd say,\n",
      "It's me, because my show's where comedy meets play.\n",
      "\n",
      "You may have the edge on policy talks galore,\n",
      "But when it comes to laughter and fun, I'm the one who scores.\n",
      "\n",
      "**Round 3: John Oliver**\n",
      "\n",
      "Not so fast, Stephen, don't count yourself in,\n",
      "We're not done yet! You think you've got the win?\n",
      "I'll give you a nod for your comedic skills so fine,\n",
      "But when it comes to substance and truth-telling that shines?\n",
      "\n",
      "The crowd goes wild as both opponents throw down their hands, acknowledging each other's strengths. In the end, there's no clear winner – just two comedy titans, each holding their own ground in this epic rap battle.\n"
     ]
    }
   ],
   "source": [
    "response_message = model.invoke(\n",
    "    \"Simulate a rap battle between Stephen Colbert and John Oliver\"\n",
    ")\n",
    "\n",
    "print(response_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a1b5d89-6be4-45d0-b0a1-0a7886f763ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main themes in these retrieved docs are:\\n\\n1. Task decomposition: The ability to break down complex tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\n2. Planning: A crucial component that involves identifying and breaking down large tasks into smaller subgoals, and then planning ahead to achieve them.\\n3. Expert models: The use of expert models to execute on specific tasks and log results, providing a way for the agent to learn from its mistakes and refine its approach over time.\\n4. Self-reflection and refinement: The ability of the agent to reflect on past actions, identify areas for improvement, and refine its approach to achieve better results.\\n\\nThese themes highlight the key aspects of an LLM-powered autonomous agent system, including planning, task decomposition, expert models, and self-refinement.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Summarize the main themes in these retrieved docs: {docs}\"\n",
    ")\n",
    "\n",
    "\n",
    "# Convert loaded documents into strings by concatenating their content\n",
    "# and ignoring metadata\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "chain = {\"docs\": format_docs} | prompt | model | StrOutputParser()\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "docs = vectorstore.similarity_search(question)\n",
    "\n",
    "chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cf2862a-ac40-4862-b2e0-6833a41ad28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are three approaches to task decomposition: (1) using a Large Language Model (LLM) with simple prompting, such as \"Steps for XYZ.\" or \"Subgoals for achieving XYZ.\", (2) using task-specific instructions like writing a story outline, and (3) with human inputs. These methods enable efficient breakdown of complex tasks into smaller subgoals.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Answer the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(context=lambda input: format_docs(input[\"context\"]))\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "docs = vectorstore.similarity_search(question)\n",
    "\n",
    "# Run\n",
    "chain.invoke({\"context\": docs, \"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac288668-f0d8-4de7-8523-eb3d39693e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "254dce18-a8ec-4155-a8c3-2609e65f7b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are three approaches to task decomposition: (1) using simple prompting with a Large Language Model (LLM), such as \"Steps for XYZ.\" or \"What are the subgoals for achieving XYZ?\", (2) using task-specific instructions, and (3) with human inputs. These approaches enable an autonomous agent system to break down large tasks into smaller, manageable subgoals.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "qa_chain.invoke(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
